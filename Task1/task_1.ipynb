{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee973bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from configs import kafka_configs, db_configs\n",
    "\n",
    "#Ініціалізація Spark та конфігурація\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,mysql:mysql-connector-java:8.0.32 pyspark-shell'\n",
    "\n",
    "# Створення SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OlympicStreaming\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars\", \"mysql-connector-java-8.0.32.jar\") \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", True)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b007aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+--------------+------+------+-----------------+-----------+--------------------+--------------------+\n",
      "|athlete_id|               name|   sex|          born|height|weight|          country|country_noc|         description|       special_notes|\n",
      "+----------+-------------------+------+--------------+------+------+-----------------+-----------+--------------------+--------------------+\n",
      "|     65649|       IvankaBonova|Female|    4April1949| 166.0|    55|         Bulgaria|        BUL|PersonalBest40053...|                 nan|\n",
      "|    112510|   NataliyaUryadova|Female|   15March1977| 184.0|    70|RussianFederation|        RUS|                 nan|ListedinOlympians...|\n",
      "|    114973|   EssaIsmailRashed|  Male|14December1986| 165.0|    55|            Qatar|        QAT|PersonalBest10000...|ListedinOlympians...|\n",
      "|     30359|          PterBoros|  Male| 12January1908|      |   nan|          Hungary|        HUN|Between1927and193...|                 nan|\n",
      "|     50557|       RudolfPiowat|  Male|   28April1900|      |   nan|   Czechoslovakia|        TCH|RudolfPiowatyjoin...|                 nan|\n",
      "|    146111|  SvetlanaKholomina|Female| 9November1997|      |   nan|              ROC|        ROC|                 nan|                 nan|\n",
      "|    133041|    VincentRiendeau|  Male|13December1996| 178.0|    68|           Canada|        CAN|                 nan|ListedinOlympians...|\n",
      "|    110425|         TanjaMorel|Female|  4October1975| 164.0|    58|      Switzerland|        SUI|                 nan|ListedinOlympians...|\n",
      "|    110705|     MaksimShabalin|  Male| 25January1982| 183.0|    76|RussianFederation|        RUS|                 nan|ListedinOlympians...|\n",
      "|     54541|            GRegter|  Male|    6March1916|      |   nan|      Netherlands|        NED|                 nan|                 nan|\n",
      "|     22721|  AristidePontenani|  Male|           nan|      |   nan|            Italy|        ITA|                 nan|                 nan|\n",
      "|     56266|       GoYeongChang|  Male|   21March1926| 167.0|    75|  RepublicofKorea|        KOR|                 nan|                 nan|\n",
      "|     82227|     MarliesRostock|Female|   20April1960|      |   nan|      EastGermany|        GDR|MarliesRostockwon...|WifedivorcedofUwe...|\n",
      "|     93334|     CraigHutchison|  Male|     26May1975| 198.0|    97|           Canada|        CAN|                 nan|ListedinOlympians...|\n",
      "|    146013|       RaquelQueirs|Female|    4March2000| 167.0|    56|         Portugal|        POR|                 nan|                 nan|\n",
      "|    109912|VyacheslavKurginyan|  Male|22December1986| 170.0|    65|RussianFederation|        RUS|                 nan|ListedinOlympians...|\n",
      "|     37019|        PhilippeLot|  Male|    13June1967| 189.0|    85|           France|        FRA|                 nan|ListedinOlympians...|\n",
      "|     22885|        RudyKugeler|  Male|  11August1928| 187.0|    86|       Luxembourg|        LUX|                 nan|                 nan|\n",
      "|     95497|  YoshihiroMiyazaki|  Male|     10May1930| 173.0|    70|            Japan|        JPN|                 nan|                 nan|\n",
      "|        76|       RoperBarrett|  Male|24November1873|      |   nan|     GreatBritain|        GBR|RoperBarrettlearn...|                 nan|\n",
      "+----------+-------------------+------+--------------+------+------+-----------------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #Зчитування біологічних даних з MySQL\n",
    "athlete_bio_df = spark.read.format('jdbc').options(\n",
    "    url=db_configs[\"url\"],\n",
    "    driver='com.mysql.jdbc.Driver',  # com.mysql.jdbc.Driver\n",
    "    dbtable=\"athlete_bio\",\n",
    "    user=db_configs[\"user\"],\n",
    "    password=db_configs[\"password\"]) \\\n",
    "    .load()\n",
    "\n",
    "athlete_bio_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d36979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фільтрація некоректних значень\n",
    "from pyspark.sql.functions import col\n",
    "bio_clean_df = athlete_bio_df \\\n",
    "    .filter(col(\"height\").cast(\"float\").isNotNull()) \\\n",
    "    .filter(col(\"weight\").cast(\"float\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "920ad315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Зчитування результатів з MySQL та запис у Kafka\n",
    "event_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_configs[\"url\"]) \\\n",
    "    .option(\"dbtable\", \"athlete_event_results\") \\\n",
    "    .option(\"user\", db_configs[\"user\"]) \\\n",
    "    .option(\"password\", db_configs[\"password\"]) \\\n",
    "    .option(\"driver\", db_configs[\"driver\"]) \\\n",
    "    .load()\n",
    "\n",
    "# Запис у Kafka\n",
    "event_df.selectExpr(\"to_json(struct(*)) AS value\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"77.81.230.104:9092\") \\\n",
    "    .option(\"topic\", \"athlete_event_results\") \\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_PLAINTEXT\") \\\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "    .option(\"kafka.sasl.jaas.config\", f\"org.apache.kafka.common.security.plain.PlainLoginModule required username='admin' password={kafka_configs['password']};\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd94a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Зчитування з Kafka та парсинг JSON\n",
    "from pyspark.sql.functions import from_json, schema_of_json\n",
    "\n",
    "# Стримінг з Kafka\n",
    "kafka_stream_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"77.81.230.104:9092\") \\\n",
    "    .option(\"subscribe\", \"athlete_event_results\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_PLAINTEXT\") \\\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "    .option(\"kafka.sasl.jaas.config\", f\"org.apache.kafka.common.security.plain.PlainLoginModule required username='admin' password={kafka_configs['password']};\") \\\n",
    "    .load()\n",
    "\n",
    "# Парсинг JSON\n",
    "sample_json = event_df.selectExpr(\"to_json(struct(*)) AS value\").first()[\"value\"]\n",
    "json_schema = schema_of_json(sample_json)\n",
    "\n",
    "parsed_df = kafka_stream_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(\"value\", json_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4280da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Об’єднання з біо-даними та агрегація\n",
    "from pyspark.sql.functions import current_timestamp, avg\n",
    "\n",
    "joined_df = parsed_df.join(bio_clean_df, on=\"athlete_id\", how=\"inner\")\\\n",
    "    .drop(parsed_df[\"country_noc\"])\n",
    "\n",
    "aggregated_df = joined_df.groupBy(\n",
    "    \"sport\", \"medal\", \"sex\", \"country_noc\"\n",
    ").agg(\n",
    "    avg(\"height\").alias(\"avg_height\"),\n",
    "    avg(\"weight\").alias(\"avg_weight\")\n",
    ").withColumn(\"calculated_at\", current_timestamp())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Стримінг у Kafka та MySQL через forEachBatch\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoint_path = str(Path.home() / \"spark_checkpoints\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "def write_to_kafka_and_db(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        print(f\"Batch {batch_id} is empty, skipping...\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        print(f\"Processing batch {batch_id}\")\n",
    "        \n",
    "        # Запис у Kafka-топік\n",
    "        batch_df.selectExpr(\"to_json(struct(*)) AS value\") \\\n",
    "            .write \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"77.81.230.104:9092\") \\\n",
    "            .option(\"topic\", \"aggregated_athlete_stats\") \\\n",
    "            .option(\"kafka.security.protocol\", \"SASL_PLAINTEXT\") \\\n",
    "            .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "            .option(\"kafka.sasl.jaas.config\", \n",
    "                   f\"org.apache.kafka.common.security.plain.PlainLoginModule required \" \\\n",
    "                   f\"username='admin' password={kafka_configs['password']};\") \\\n",
    "            .save()\n",
    "\n",
    "        # Запис у БД MySQL\n",
    "        batch_df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", db_configs[\"url\"]) \\\n",
    "            .option(\"dbtable\", \"aggregated_athlete_stats\") \\\n",
    "            .option(\"user\", db_configs[\"user\"]) \\\n",
    "            .option(\"password\", db_configs[\"password\"]) \\\n",
    "            .option(\"driver\", db_configs[\"driver\"]) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "            \n",
    "        print(f\"Successfully processed batch {batch_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "# Стримінг\n",
    "query = aggregated_df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(write_to_kafka_and_db) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .option(\"spark.sql.streaming.minBatchesToRetain\", \"2\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "try:\n",
    "    print(\"Starting streaming query...\")\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopping stream...\")\n",
    "    query.stop()\n",
    "    print(\"Stream stopped\")\n",
    "except Exception as e:\n",
    "    print(f\"Stream failed: {str(e)}\")\n",
    "    query.stop()\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
